{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thush-ar/fake-product-review-analyzer/blob/main/fake_product_review_analyzer_using_randomforest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit pandas scikit-learn joblib nltk pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfVqd7i6iR-U",
        "outputId": "ee25311b-1415-4059-add5-beed862e22f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.8.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.4.0 streamlit-1.50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_age88diP6U",
        "outputId": "e009cd19-cfd6-4670-f98e-623e61095268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import os\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Fake Review Detection\",\n",
        "    page_icon=\"ðŸ¤–\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# --- NLTK Setup ---\n",
        "# Download necessary NLTK data (this will be cached by Streamlit)\n",
        "@st.cache_resource\n",
        "def download_nltk_data():\n",
        "    try:\n",
        "        stopwords.words('english')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "    try:\n",
        "        nltk.data.find('corpora/wordnet.zip')\n",
        "    except LookupError:\n",
        "        nltk.download('wordnet')\n",
        "download_nltk_data()\n",
        "\n",
        "# --- Text Preprocessing Function ---\n",
        "# This function is cached to speed up repeated runs\n",
        "@st.cache_data\n",
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# --- Model Training Function ---\n",
        "# Caching the whole training process to avoid re-training on every interaction\n",
        "@st.cache_data\n",
        "def train_model(df):\n",
        "    # Map labels to binary (CG = Genuine, OR = Fake)\n",
        "    df['label'] = df['label'].map({'CG': 0, 'OR': 1})\n",
        "    df.dropna(subset=['text_', 'label'], inplace=True)\n",
        "\n",
        "    df['processed_text'] = df['text_'].apply(preprocess_text)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X = vectorizer.fit_transform(df['processed_text'])\n",
        "    y = df['label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=['Genuine (CG)', 'Fake (OR)'])\n",
        "\n",
        "    # Save the model and vectorizer\n",
        "    joblib.dump(model, 'fake_review_model.pkl')\n",
        "    joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "# --- Main App Interface ---\n",
        "st.title(\"ðŸ¤– Fake Review Detection System\")\n",
        "st.markdown(\"Upload a dataset, train a Random Forest model, and classify new reviews.\")\n",
        "\n",
        "# --- Step 1: Upload Dataset ---\n",
        "st.header(\"1. Upload Your Dataset\")\n",
        "st.markdown(\"The CSV file must contain a `text_` column for the review text and a `label` column with 'CG' for genuine and 'OR' for fake reviews.\")\n",
        "uploaded_file = st.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    try:\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "        st.success(\"Dataset loaded successfully!\")\n",
        "        st.dataframe(df.head())\n",
        "\n",
        "        # --- Step 2: Train Model ---\n",
        "        st.header(\"2. Train the Classification Model\")\n",
        "        if st.button(\"Click to Train Model\"):\n",
        "            with st.spinner('Training in progress... This may take a moment.'):\n",
        "                accuracy, report = train_model(df)\n",
        "                st.session_state['model_trained'] = True\n",
        "                st.session_state['accuracy'] = accuracy\n",
        "                st.session_state['report'] = report\n",
        "\n",
        "            st.success(\"Model trained successfully!\")\n",
        "            st.metric(label=\"Model Accuracy\", value=f\"{st.session_state['accuracy']:.4f}\")\n",
        "            st.text(\"Classification Report:\")\n",
        "            st.code(st.session_state['report'])\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "        st.warning(\"Please ensure your CSV has 'text_' and 'label' columns.\")\n",
        "\n",
        "# --- Step 3: Classify New Review ---\n",
        "if st.session_state.get('model_trained', False):\n",
        "    st.header(\"3. Classify a New Review\")\n",
        "    user_input = st.text_area(\"Enter a review text to classify:\", height=150)\n",
        "\n",
        "    if st.button(\"Classify Review\"):\n",
        "        if user_input.strip():\n",
        "            # Load the saved model and vectorizer\n",
        "            try:\n",
        "                model = joblib.load('fake_review_model.pkl')\n",
        "                vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "                # Preprocess and transform the user input\n",
        "                processed_input = preprocess_text(user_input)\n",
        "                vectorized_input = vectorizer.transform([processed_input])\n",
        "\n",
        "                # Make prediction\n",
        "                prediction = model.predict(vectorized_input)\n",
        "                prediction_proba = model.predict_proba(vectorized_input)\n",
        "\n",
        "                # Display result\n",
        "                if prediction[0] == 1:\n",
        "                    st.error(f\"**Prediction: FAKE Review** (Confidence: {prediction_proba[0][1]:.2%})\")\n",
        "                else:\n",
        "                    st.success(f\"**Prediction: GENUINE Review** (Confidence: {prediction_proba[0][0]:.2%})\")\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                st.error(\"Model files not found. Please train the model first.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred during prediction: {e}\")\n",
        "        else:\n",
        "            st.warning(\"Please enter a review to classify.\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a dataset and train the model to start classifying reviews.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "# Replace with your token from dashboard\n",
        "ngrok.set_auth_token(\"32lQofxVE9Ujp64uPPcC9SzA0hG_2rcUYzR6gxhbTdKzNsWJf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7henuenkiiOt",
        "outputId": "00bd0260-515b-41d2-acd7-905e449cf25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a tunnel to the Streamlit port\n",
        "public_url = ngrok.connect(8501)   # integer, no keyword\n",
        "print(\"Streamlit public URL:\", public_url)\n",
        "\n",
        "# Launch Streamlit app\n",
        "!streamlit run app.py &>/dev/null &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbkDZPpsjZTY",
        "outputId": "f8b016b0-64cf-489c-a3b2-48d9416976cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit public URL: NgrokTunnel: \"https://09ac5dc77a99.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}